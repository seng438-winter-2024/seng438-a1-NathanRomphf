>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 21      |
|-----------------|
| Samir Sakr               |   
| Mason Bullock               |   
| Nathan Romphf               |   
| Sanika Shendye               |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#_Toc439194677)

[2 High-level description of the exploratory testing plan	1](#_Toc439194678)

[3 Comparison of exploratory and manual functional testing	1](#_Toc439194679)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

The purpose of this lab was to gain a deeper understanding of software testing through exploration of the system under test (SUT), exploratory testing, manual scripted testing, and regression testing. 

The first version of the system was tested firstly using exploratory testing, and then manual functional testing. The updated version of the system was then tested again against the manual functional tests, as a way of undergoing system regression testing to ensure that new updates to the SUT did not introduce more bugs.

Prior to the lab, we knew that exploratory testing was unscripted or unplanned testing. This is system testing with minimal structure, and is deemed exploratory because of the exploratory act of finding bugs and defects, without a structured plan to follow. 

 We knew that manual functional testing involved following a devised plan of test cases manually. This is similar to testing the functionality of a system in a structured way, and ensuring that the actual and expected outputs match. 

# High-level description of the exploratory testing plan

The exploratory testing plan that we followed was to try to test all of the SUT’s major functionalities, in the realm that a normal ATM user would use the system. This allowed us to ensure that all of the functionalities and cases that a regular user would undergo would function as expected. Exploratory testing was conducted for 30 minutes to explore the SUT’s functionality and check for any immediate defects. 

We checked the functionalities to login with the correct pin and card number, as well as cases when the login information was entered erroneously. 

We checked the withdrawal functionality to check the SUT’s behaviour in instances where the ATM had enough cash to dispense and the account had sufficient funds, as well as cases where the ATM did not have sufficient cash, and instances where the withdrawal amount was over the amount in the account. 

We tested the deposit functionality to check that the SUT was correctly updating the account balance with the amount that was deposited. 

We tested the transfer function to ensure that the correct balances were transferred between accounts, and that only valid transfers were able to be completed. 

We tested the balance inquiry functionality to ensure that the displayed balances were accurate between other transactions, and accurately displayed the account balances. 


# Comparison of exploratory and manual functional testing

Exploratory testing is a software testing approach that simultaneously allows the tester to design and execute tests as they explore the system. This approach involved following a cursory testing plan of how to explore the system, and making notes of any bugs or discrepancies from expected behaviour while using the application. This is considered an unscripted or unstructured approach to system testing. There are no predefined test cases, and testers are able to adapt their testing and test cases to the system in real time. The documentation for this approach is limited, and relies largely on the testers documentation of bugs that they have found in real time. This method is beneficial because it supports creativity and use of the system, but trades off in terms of defined structure and efficiency, because thorough exploratory testing would take a significant amount of time. 

Manual functional testing refers to the act of testing the functionality of a system against the expected behaviour of a system to verify that the features of the system work as intended. The test cases and steps to replicate the behaviour are planned systematically, and the tests are run against their expected outcomes and then classified as passing the test or failing it. This approach is structured and scripted. There is extensive documentation of the test cases with this approach, as the test cases to be run against the system are documented. This approach is high in reproducibility, as the results of the test can be reproduced by following the documentation. This method is beneficial because it allows for functionality of the system to be tested in a structured manner, with efficiency, but could potentially trade off in terms of effectiveness if the test cases are not thorough. 

Both testing approaches rely on humans to execute the tests, and can be used to ensure that the system meets user requirements. A combination of both exploratory and manual functional testing can be used to comprehensively test a system and its behaviour. 


-   Note that you need to submit a report generated by your defect tracking
    system, containing all defects recorded in the system.

# Notes and discussion of the peer reviews of defect reports

The defects found in the system were peer reviewed by the teams after the exploratory and manual functional tests were run. Many of the same bugs were found by the teams, but there were several unique defects that were encountered after peer review. The defect reports were structured similarly in the tracking tool, due to the adherence to the assignment guidelines. When doing the peer review, we were able to replicate the reported defects by using the information reported by the other team, and compare to the output that was reported. 

# How the pair testing was managed and team work/effort was divided 

Pair testing was managed by splitting the team into two groups of 2 where one person in each group was the driver and the other was the passenger/bug report writer. The driver was the one responsible for working with the software and in the case of manual testing, reading the manual test cases and reproducing the steps to see if a bug arised. The passenger would watch the driver to ensure no mistakes are made and then would write a bug report of what went wrong in Azure. We split up the work 50-50 between the two pairs and met up to go over it all together as a team afterwards. Once that was done we worked on writing the report and compiling the bug reports.

# Difficulties encountered, challenges overcome, and lessons learned

One of the major difficulties that we encountered was the use of the Azure DevOps software to track bugs. We had never used the software and were unfamiliar with the settings, permissions, and bug tracking tool behaviour. We were able to overcome this challenge by taking the time to learn how to use the tool, through the use of online documentation, tutorial videos, and help from TA’s in the lab. This allowed us to build familiarity with the DevOps platform and learn how to effectively use the platform. 

Another difficulty we encountered was trying to generate a readable defect report from the list we created in Azure DevOps. Again, this was something that we had never been asked to do before so we had some difficulties figuring it out. To overcome these difficulties we read an article on how to use the queries function, and asked for assistance from TA’s when the .csv file that was generated displayed the data in an unreadable single line of HTML code. From this we were able to learn how to generate a spreadsheet from the bug tracking tool, and how to use that spreadsheet to create a nicely formatted table in a markdown file.

# Comments/feedback on the lab and lab document itself

The lab document initially looked very daunting due to the sheer length of the readme file. In addition, it took a while to fully understand what the lab was requesting of us, but once we figured it out it was quite straightforward. Maybe splitting up the 40 manual testing methods table from the instructions would be beneficial. The lab itself was straightforward and the instructions themselves were informative. The overall experience with this lab was positive. 

